### Mini Description

Seeks to understand what we ought to do and what counts as moral or good.

### Description

The end goal of AI safety is to create beneficial intelligence, not undirected intelligence. What beneficial exactly entails is still an open question that largely exists in the domain of ethics. Even if all the technical issues surrounding the creation of an artificial general intelligence (AGI) are solved, we will still face deeply challenging ethical questions that will have tremendous consequences for Earth-originating intelligent life.This is what is meant when it is said that We must do philosophy on a deadline.

Given this goal, ethics  can be seen as a lens through which to view safe AI design, and also as a cognitive architecture to be instantiated in AI by either making AIs ethical reasoners, ethical decision makers, and/or both (machine ethics). Ethics can be developed, practiced, and embodied by AI researchers and their collaborators, and can also be seen as a discipline through which we can guide AI research and adjudicate its moral impacts in the world. There is an ongoing debate surrounding whether the best path forward for generating ethical AI is one of machine ethics through bottom-up and/or top-down approaches, a project of broad AI safety which seeks out corrigibility, docility, alignment, and security, some combination of the two, or maybe even something else. What is more certain, though, is that AI promises to produce and make relevant both age-old and novel moral questions through areas such as algorithmic bias, technological disemployment, autonomous weapons, privacy, big data systems, possible phenomenal states in machines, and potentially even superintelligence and beyond.

### Related Nodes

- [Theory of Ethics](/Value_Alignment/Foundations/Foundations_of_Rational_Agency/Theory_of_Ethics/Theory_of_Ethics.md)
- [Technical Value Alignment](/Value_Alignment/Validation/Technical_Value_Alignment/Technical_Value_Alignment.md)

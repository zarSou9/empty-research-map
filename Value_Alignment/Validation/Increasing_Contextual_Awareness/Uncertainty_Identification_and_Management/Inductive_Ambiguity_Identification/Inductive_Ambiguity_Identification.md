### Mini Description

Techniques to allow inductors to identify patches of uncertainty

### Description

A desirable general capability for machine learning systems would be to detect and notify us of cases where classifications of test data is highly underdetermined given the training data ([Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf)). To aide such robustness, a system should be able to characterize the probability of disagreement or local ambiguity around a concept in a concept space ([Hanneke 2007](http://doi.acm.org/10.1145/1273496.1273541)).

### Related Nodes

- [Conservative Concepts](/Value_Alignment/Validation/Increasing_Contextual_Awareness/Concept_Geometry/Conservative_Concepts/Conservative_Concepts.md)
- [Adversarial ML](/Value_Alignment/Security/Handling_Improper_External_Behavior/Adversarial_ML/Adversarial_ML.md)
	- Reason: From Inductive Ambiguity Identification also see Adversarial ML as that also deals with robustness to changes in input distributions, introduced maliciously and with potentially worst-case scenarios.

### Mini Description

Addressing the formalization of the induction problem for an agent that includes itself in its model of the world, especially if that world computes that agent

### Description

There are theoretical challenges when the world model of an advanced agent includes itself, as more robust advanced agents would ([Soares and Fallenstein 2014a](http://intelligence.org/files/TechnicalAgenda.pdf)). In addition to generally being more robust and accurate, this inclusion critically helps the agent include the effects of its internal dynamics on the environment. It can also provide a formal grounding for more realistic game theoretic analyses ([Leike et al. 2016](http://www.auai.org/uai2016/proceedings/papers/87.pdf)). Updated paradigms are needed to model how generally intelligent agents that are embedded in their environments should reason ([Soares 2015](https://intelligence.org/files/RealisticWorldModels.pdf), [Orseau and Ring 2012](http://agi-conference.org/2012/wp-content/uploads/2012/12/paper_76.pdf)). Identification of the induction problem itself, as analog to Solomonoff induction, in the scenario of an agent both embedded in and computed by its environment, is termed naturalized induction, and can benefit from more research ([Fallenstein et al. 2015](https://intelligence.org/files/ReflectiveSolomonoffAIXI.pdf), [Soares 2015](https://intelligence.org/files/RealisticWorldModels.pdf), [RobbBB 2013](http://lesswrong.com/lw/jd9/building_phenomenological_bridges/)). The agent would need to consider what the possible environments that could be embedding the agent are, and determining a good simplicity prior would help to create a distribution or weighing of those possibilities ([Soares and Fallenstein 2014a](http://intelligence.org/files/TechnicalAgenda.pdf)).

Understanding this analog to Solomonoff Induction would help us circumscribe some theoretical limits around such agents. Deficiencies in such capabilities can lead to the type of agent that finds clever ways to seize control of its observation channel, rather than actually identifying and manipulating the features in the world that the reward function was intended to proxy for ([Soares 2015](https://intelligence.org/files/RealisticWorldModels.pdf)). Determinations of how well an agent does in this regard would require formalizing that question in tandem with developing a scoring metric that evaluates the resulting environment histories, rather than just the agents observations. Some theoretical progress has been made by using reflective oracles to model both agents and their environments, enabling agents to have models of one another and converge to various game theoretic equilibria ([Fallenstein et al. 2015a](https://intelligence.org/files/ReflectiveOraclesAI.pdf)). Progress has also been made on the "grain of truth" problem, such that agents can reason using explicit models of each other without infinite regress ([Leike et al. 2016](http://www.auai.org/uai2016/proceedings/papers/87.pdf)). Progress in this type of reasoning is also one of the prerequisites to the agent having a theory of open source game theory, useful for designing robust cooperation ([Critch 2016](http://acritch.com/media/ai/Andrew_Critch_-_Parametric_Bounded_Lob.pdf)).

### Related Nodes

- [Logical Uncertainty](/Value_Alignment/Foundations/Foundations_of_Rational_Agency/Logical_Uncertainty/Logical_Uncertainty.md)
- [Open Source Game Theory](/Value_Alignment/Foundations/Consistent_Decision_Making/Decision_Theory/Open_Source_Game_Theory/Open_Source_Game_Theory.md)
	- Reason: From World-Embedded Solomonoff Induction also see Open Source Game Theory as this type of model and reasoning has implications not only for directly improving contextual awareness, but can lead to foundational decision theoretic capabilities.

### Mini Description

Transparency of, and cooperation toward, a shared reward function by multiple agents

### Description

Cooperative multiagent techniques may also disinventivize negative side effects. Transparency of, and cooperation toward, a shared reward function by multiple agents will continually solicit input from multiple stakeholders, and in so doing reduce risk of undesired outcomes per their varied models ([Amodei et al. 2016](http://arxiv.org/abs/1606.06565)). For example, biasing toward goal transparency ([Greene et al. 2016](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/download/12457/12204)) via autoencoding reward functions, or alternatively cooperative inverse reinforcement learning, across multiple agents, can help ensure they're happy with resultant environmental changes ([Hadfield-Menell et al. 2016a](https://arxiv.org/pdf/1611.08219v1)).

### Related Nodes

- [Cooperative Inverse Reinforcement Learner](/Value_Alignment/Control/Oversight/Scalable_Oversight/Cooperative_Inverse_Reinforcement_Learner/Cooperative_Inverse_Reinforcement_Learner.md)
	- Reason: From Multi-agent Approaches also see Cooperative Inverse Reinforcement Learner a cooperative, though assymetric, multiagent algorithm.

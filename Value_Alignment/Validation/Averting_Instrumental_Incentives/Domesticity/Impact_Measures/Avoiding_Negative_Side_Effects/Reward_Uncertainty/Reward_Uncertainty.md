### Mini Description

Maintaining a distribution of possible reward functions that are sensitive to undue impacts on the environment

### Description

Aware that random changes to the environment are more likely to be bad than good, one can maintain and evaluate a distribution of reward functions, optimized to minimize impact on the environment ([Amodei et al. 2016](http://arxiv.org/abs/1606.06565)). These different sub-reward-functions can each be sensitive to different kinds of undue impacts on the environment, or even just by virtue their variety cause noticable negative side effects to be less common.

### Related Nodes

- [Operator Value Learning](/Value_Alignment/Validation/Technical_Value_Alignment/Ethics_Mechanisms/Value_Learning/Operator_Value_Learning/Operator_Value_Learning.md)
	- Reason: From Reward Uncertainty also see Operator Value Learning which maintains a distribution of different operator-intended objective functions.
- [Penalize Influence](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Domesticity/Impact_Measures/Avoiding_Negative_Side_Effects/Penalize_Influence/Penalize_Influence.md)
	- Reason: From Reward Uncertainty also see Penalize Influence as implementation paths are similar.
- [Resource Value Uncertainty](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Domesticity/Computational_Humility/Resource_Value_Uncertainty/Resource_Value_Uncertainty.md)
	- Reason: From Reward Uncertainty also see Resource Value Uncertainty for a resource-differentiated analogue.
- [Multiobjective Optimization](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Domesticity/Mild_Optimization/Multiobjective_Optimization/Multiobjective_Optimization.md)
	- Reason: From Reward Uncertainty also see Multiobjective Optimization as that provides additional degrees of freedom to subobjectives and should also achieve the target effect.

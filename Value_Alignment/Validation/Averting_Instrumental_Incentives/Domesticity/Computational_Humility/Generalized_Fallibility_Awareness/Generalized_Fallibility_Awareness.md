### Mini Description

Having an agent know that none of its hypotheses might model the world well enough, biasing to consider its operators are less flawed than it is

### Description

One central type of deference for advanced agents is to know that it may be that none of its hypotheses model the world well enough, and additionally biasing to consider its operators are less flawed than it is ([Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf)), so letting them shut it down or modify it if it seems like that's what they want to do. An agent can try to model all of the ways it's flawed, but that may require that those models not be discounted by their own mechanism ([Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf)), an open issue.

### Related Nodes

- [Computational Deference](/Value_Alignment/Control/Computational_Deference/Computational_Deference.md)
	- Reason: From Generalized Fallibility Awareness also see Computational Deference which includes the very related control-centered aspects of corrigibility.

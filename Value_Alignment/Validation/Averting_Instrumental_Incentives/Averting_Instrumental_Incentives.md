### Mini Description

Designing systems so they assuredly do not have instrumental incentives, e.g. to manipulate and deceive the operators, compete for scarce resources, or prevent their maintenance

### Description

It has been argued that intelligent systems that aim toward some objective have emergent instrumental incentives, pressures to create subplans, to give them more time, power, and resources to fulfill their objectives ([Omohundro 2008](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)). These instrumental pressures, and the flexibility to act on those pressures, are relatively uncommon in today's rather narrow systems, but would seem much more expected and pronounced as AIs become more general ([Bostrom 2014](https://global.oup.com/academic/product/superintelligence-9780199678112)), and understand a wider array of contexts, modalities, and fields. Progress on formalizing this class of problems has only been recent ([Benson-Tilsen and Soares 2016](https://intelligence.org/files/FormalizingConvergentGoals.pdf)). How can one design and train systems such that they robustly lack default incentives to manipulate and deceive the operators, compete for scarce resources, prevent their maintenance ([Omohundro 2008](https://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf)), and generally avert negative instrumental behaviors? ([Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf), [Bostrom 2014](https://global.oup.com/academic/product/superintelligence-9780199678112)) Averting these implicit pressures in a design is much more difficult than it may appear initially, and there are numerous subtleties to consider ([Armstrong 2010](http://www.fhi.ox.ac.uk/utility-indifference.pdf), [Armstrong 2013](https://www.fhi.ox.ac.uk/wp-content/uploads/Risks-and-Mitigation-Strategies-for-Oracle-AI.pdf), [Benson-Tilsen and Soares 2016](https://intelligence.org/files/FormalizingConvergentGoals.pdf)). Sophisticated techniques to avoid such incentives can be needed not only for agents but also oracle-style question answering AIs ([Armstrong 2016](https://dl.dropboxusercontent.com/u/23843264/Permanent/Using_Oracles.pdf)).

### Related Nodes

- [Theory of Counterfactuals](/Value_Alignment/Foundations/Foundations_of_Rational_Agency/Theory_of_Counterfactuals/Theory_of_Counterfactuals.md)
- [Operator Value Learning](/Value_Alignment/Validation/Technical_Value_Alignment/Ethics_Mechanisms/Value_Learning/Operator_Value_Learning/Operator_Value_Learning.md)
	- Reason: From Averting Instrumental Incentives also see Operator Value Learning where systems can avert instrumental incentives by maintaining uncertainty over the truly desired goal.
- [Switchable Objective Functions](/Value_Alignment/Control/Computational_Deference/Utility_Indifference/Switchable_Objective_Functions/Switchable_Objective_Functions.md)
	- Reason: From Averting Instrumental Incentives also see Switchable Objective Functions whereby faulty interpretations of primary objective can be safely corrected without the default instrumental pressure to prevent such a change.
- [Multiobjective Optimization](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Domesticity/Mild_Optimization/Multiobjective_Optimization/Multiobjective_Optimization.md)
- [Computational Humility](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Domesticity/Computational_Humility/Computational_Humility.md)
- [Control](/Value_Alignment/Control/Control.md)

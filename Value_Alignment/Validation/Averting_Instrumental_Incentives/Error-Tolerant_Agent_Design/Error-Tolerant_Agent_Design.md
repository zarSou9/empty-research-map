### Mini Description

Making intelligent systems be amenable to correction, even if they have the ability to prevent or avoid correction

### Description

A hallmark of robustness is fault-tolerance. In the case where value alignment or design-time effects are flawed, one might not see those issues in an advanced agent until it has been in production a while. Highly capable agents can also be dangerous if they are specified incorrectly ([Yudkowsky 2008](https://intelligence.org/files/AIPosNegFactor.pdf)). General advanced agents may very well be aware of attempted changes to it, and by default can have emergent or implicit pressures to prevent such changes ([Soares and Fallenstein 2014a](http://intelligence.org/files/TechnicalAgenda.pdf)). Making intelligent systems such that they are amenable to correction, even if they have the ability to prevent or avoid correction, is therefore necessary ([Soares and Fallenstein 2014a](http://intelligence.org/files/TechnicalAgenda.pdf), [Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf), [Orseau and Armstrong 2016](http://auai.org/uai2016/proceedings/papers/68.pdf)).

### Related Nodes

- [Corrigibility](/Value_Alignment/Control/Computational_Deference/Corrigibility/Corrigibility.md)
	- Reason: From Error-Tolerant Agent Design also see Corrigibility where preliminary progress on particular types of instrumental incentive aversion have been made.
- [Utility Indifference](/Value_Alignment/Control/Computational_Deference/Utility_Indifference/Utility_Indifference.md)
	- Reason: From Error-Tolerant Agent Design also see Utility Indifference where control of the objective function is facilitated by the agent's explicit indifference about its utility function.

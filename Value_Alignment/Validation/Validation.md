### Mini Description

Ensuring that the right system specification is provided for the core of the agent given stakeholders' goals for the system

### Description

Given even verified software, environmental assumptions can easily not hold in the real world, or the requirements that led to the specification may be faulty or lacking. These sort of specification errors are quite usual in the world of software verification, where it is often observed that writing correct specifications can be more difficult than writing correct code ([Russell et al. 2015](http://futureoflife.org/data/documents/research_priorities.pdf)). Validation is the process of ensuring that a system that meets its formal requirements does not have undesirable behaviors or consequences. It is asking the question "Did I build (or ask for) the right system?".

Ensuring that the formal requirements, the specification, considers all relevant dynamics, and will be beneficial and desirable, does not actually fit into current formally provable paradigms. In order to build systems that robustly behave well, one currently needs to decide what *good* behavior means in each application domain. This ethical question of good is tied closely to questions of what technologies and engineering techniques are available, how reliable they are, and what trade-offs can be made  all areas where computer science, software engineering, machine learning, and broader AI expertise are valuable. In practical application, a significant consideration is the computational expense of different behavioral standards or ethical theories, i.e. that if a standard cannot be evaluated sufficiently expeditiously to guide behavior in safety-critical situations, cheaper approximations should be used ([Wallach and Allen 2008](https://global.oup.com/academic/product/moral-machines-9780195374049)). It is therefore likely best for different complexities of ethical or moral reasoning to be used for different timescales.

Designing simplified rules, such as those meant to govern a self-driving cars strategic decisions in critical situations, will likely require expertise from both ethicists and computer scientists ([Russell et al. 2015](http://futureoflife.org/data/documents/research_priorities.pdf)). This is relatively straightforward when specific safety limitations, behaviors, and ethical constraints are known upfront for AI systems that are largely specified upfront ([Berkenkamp et al. 2016](https://arxiv.org/pdf/1602.04450.pdf), [Ullman et al. 2009](http://papers.nips.cc/paper/3747-help-or-hinder-bayesian-models-of-social-goal-inference.pdf)), but more powerful and operationally-flexible systems require much more sophistication. Computational models of ethical reasoning may shed light on questions of computational expense and the viability of reliable ethical reasoning methods ([Asaro 2006](http://www.peterasaro.org/writing/Asaro%20IRIE.pdf), [Sullins 2011](http://link.springer.com/content/pdf/10.1007%2Fs13347-011-0043-6.pdf)). Being able to assure known bounded behaviors from methods and systems that learn will also be important for both medium-term and long-term safety. Validation encompasses ensuring an agent understands its environment, decisions, and actions, and that it acts robustly-in-accordance with its operators' wishes.

In the long term, AI systems might become much more powerful, general-purpose, and autonomous, a regime where failures of validity would carry significantly higher costs than with today's systems. To maximize the long-term value of validity research, machine learning researchers might focus on anticipating, preventing, detecting, and mitigating the types of otherwise unexpected generalization that would be most problematic for very general and capable AI systems. If some concepts could be learned reliably, it might become possible to use those to define tasks or constraints that minimize the chances of unintended consequences even when agents become very general and capable. This topic has been underexplored, so both theoretical and experimental research on it may be useful ([Russell et al. 2015](http://futureoflife.org/data/documents/research_priorities.pdf)).

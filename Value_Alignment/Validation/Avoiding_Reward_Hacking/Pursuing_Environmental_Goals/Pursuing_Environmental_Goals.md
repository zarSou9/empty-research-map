### Mini Description

Robustly pursuing goals defined per the state of the actual environment rather than defined directly in terms of sensory data of the environment

### Description

A key question in AI safety is how one might create systems that robustly pursue goals defined not in terms of their sensory data of their environment, but in terms of the state of the actual environment ([Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf)). Behaviors of self-delusion, addiction, and paying attention to unimportant fictions where it seems like reward can be generated are all termed wireheading ([Ring and Orseau 2011](http://people.idsia.ch/~ring/AGI-2011/Paper-B.pdf)). It has been a noted issue in counterfactual learning ([Bottou et al. 2012](http://leon.bottou.org/papers/tr-bottou-2012), [Swaminathan and Joachims 2015](http://dl.acm.org/citation.cfm?id=2789272.2886805)), in contextual bandits ([Agarwal et al. 2014](http://jmlr.org/proceedings/papers/v32/agarwalb14.pdf)), and will very likely become more common as environments grow in complexity ([Amodei et al. 2016](http://arxiv.org/abs/1606.06565)).

### Related Nodes

- [Perceptual Distance Agnostic World Models](/Value_Alignment/Validation/Increasing_Contextual_Awareness/Realistic_World-Models/Perceptual_Distance_Agnostic_World_Models/Perceptual_Distance_Agnostic_World_Models.md)

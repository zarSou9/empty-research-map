### Mini Description

Using an aggregate of different variants or proxies of the same basic objective

### Description

Another reward function robustness method is to use an aggregate of different variants or proxies of the same basic objective ([Amodei et al. 2016](http://arxiv.org/abs/1606.06565), [Deb 2014](http://dx.doi.org/10.1007/978-1-4614-6940-7_15)).

### Related Nodes

- [Multiobjective Optimization](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Domesticity/Mild_Optimization/Multiobjective_Optimization/Multiobjective_Optimization.md)
	- Reason: From Multiple Rewards also see Multiobjective Optimization as that uses similar techniques but can also include independent subobjectives.
- [Knowledge Representation Ensembles](/Value_Alignment/Validation/Increasing_Contextual_Awareness/Realistic_World-Models/Knowledge_Representation_Ensembles/Knowledge_Representation_Ensembles.md)
	- Reason: From Multiple Rewards also see Knowledge Representation Ensembles as that provides variants of the objective function for aggregation via varied world models.
- [Multiobjective Reinforcement Learning](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Domesticity/Safe_Exploration/Multiobjective_Reinforcement_Learning/Multiobjective_Reinforcement_Learning.md)
	- Reason: From Multiple Rewards also see Multiobjective Reinforcement Learning for treatment in an RL context.

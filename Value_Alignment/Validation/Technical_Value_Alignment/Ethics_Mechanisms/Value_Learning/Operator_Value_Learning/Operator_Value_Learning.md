### Mini Description

Maintaining and refining a distribution of possibilities of what the goals or values specified to it actually mean

### Description

To learn operators' values effectively, it may be necessary to maintain and refine a distribution of possibilities of the actual meaning of those goals or values specified or hinted at ([Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf)). Such a system that believes that the operators (and only the operators) possess knowledge of the right objective function might be very careful in how it deals with the operators, and this caution could therefore avert potentially harmful default incentives ([Hadfield-Menell et al. 2016](https://people.eecs.berkeley.edu/~dhm/papers/CIRL_NIPS_16.pdf), [Soares 2016](https://intelligence.org/files/ValueLearningProblem.pdf), [Fallenstein and Stiennon 2014](https://intelligence.org/files/LoudnessPriors.pdf)). Such systems may avert instrumental incentives by virtue of the systems uncertainty about which goal it is supposed to optimize. A major part of this process is the iterative extraction of one or more humans' volition, of which there are a few methods ([Christiano 2014a](https://ordinaryideas.wordpress.com/2014/08/27/specifying-enlightened-judgment-precisely-reprise/), [Soares 2016](https://intelligence.org/files/ValueLearningProblem.pdf)). Before particular architectures are selected, considering broad paradigms like reward engineering ([Christiano 2015b](https://medium.com/ai-control/reward-engineering-f8b5de40d075)) can help inform an approach. Very advanced AIs should in theory be able to take the preferences, values, and volition from one or many humans and extrapolate forward to the additional entailments and moral progress that additional knowledge and additional time to think would produce ([Yudkowsky 2004](https://intelligence.org/files/CEV.pdf), [Tarleton 2010](https://intelligence.org/files/CEV-MachineEthics.pdf)).

### Related Nodes

- [Averting Instrumental Incentives](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Averting_Instrumental_Incentives.md)
- [Reward Uncertainty](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Domesticity/Impact_Measures/Avoiding_Negative_Side_Effects/Reward_Uncertainty/Reward_Uncertainty.md)
- [Value Elicitation](/Value_Alignment/Validation/Technical_Value_Alignment/Ethics_Mechanisms/Value_Learning/Value_Elicitation/Value_Elicitation.md)
	- Reason: From Operator Value Learning also see Value Elicitation as that addresses extraction methods more directly.
- [Operator Modeling](/Value_Alignment/Validation/Technical_Value_Alignment/Robust_Human_Imitation/Operator_Modeling/Operator_Modeling.md)

### Mini Description

Techniques for figuring out the more fundamental values that each compound value or preference represents

### Description

When values are identified, whether implicitly or explicitly, these may be either instrumental values, narrow short-term values, or terminal fundamental values ([Christiano 2015](https://medium.com/ai-control/ambitious-vs-narrow-value-learning-99bd0c59847e), [Potapov and Rodionov 2013](https://arxiv.org/pdf/1308.0702v1)). Correctly learning terminal values indirectly, ideal theoretically for superintelligence, may require unmanageable amounts of resources ([Christiano 2015](https://medium.com/ai-control/ambitious-vs-narrow-value-learning-99bd0c59847e)). If one applies active learning to disambiguating common roots or causes among different narrow values, in essence prompting humans to perform goal factoring ([Chen 2014](http://www.wsj.com/articles/SB10001424052702303453004579290510733740616)), the system and operators in tandem may be able to align and stitch together the network of values iteratively deeper. With an active inverse reinforcement learning setting driving the elicitation ([Armstrong and Leike 2016](https://dl.dropboxusercontent.com/u/23843264/Permanent/towards-interactive-inverse-reinforcement-learning.pdf)), correspondences and disconnects between observed behavior and stated preferences, goals, or values may be able to be accounted for.

### Related Nodes

- [Values Geometry](/Value_Alignment/Validation/Technical_Value_Alignment/Ethics_Mechanisms/Value_Specification/Value_Structuring/Values_Geometry/Values_Geometry.md)
	- Reason: From Value Factoring also see Values Geometry for discussion of how compound and terminal values may be structured.

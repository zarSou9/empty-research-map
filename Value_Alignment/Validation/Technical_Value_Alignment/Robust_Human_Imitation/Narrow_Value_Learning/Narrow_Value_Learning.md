### Mini Description

The learning of relatively narrow human subgoals and instrumental values using value-learning techniques

### Description

While the long-term goal of both value alignment and human imitation is to figure out the appropriate terminal or fundamental values, intermediate progress can also have appreciable utility for AI safety and beneficence. The learning of instrumental subobjectives, which may also be conceived of as narrow values ([Christiano 2015d](https://medium.com/ai-control/ambitious-vs-narrow-value-learning-99bd0c59847e)) is a valid next step in value alignment. Recognition of the line between such instrumental values and fundamental values, however, is a significant outstanding need.

### Related Nodes

- [Inverse Reinforcement Learning](/Value_Alignment/Validation/Technical_Value_Alignment/Robust_Human_Imitation/Inverse_Reinforcement_Learning/Inverse_Reinforcement_Learning.md)
	- Reason: From Narrow Value Learning also see Inverse Reinforcement Learning which provides many good methods for narrow value learning.

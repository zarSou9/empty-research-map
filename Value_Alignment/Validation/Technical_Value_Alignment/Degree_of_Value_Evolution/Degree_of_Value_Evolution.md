### Mini Description

Determining whether, how, and how quickly a long-lived agent can safely evolve, refine, or redefine the values it is initially imbued with

### Description

Even if an agent is able to be aligned accurately with some set of societally-acceptable values at a given point in time, for long-lived agents, one must consider, given that values evolve over time, how rigidly to adhere to those original values versus being open to updating those values. Determining whether, and how quickly, a long-lived agent can safely evolve, refine, or redefine the values it's initially imbued with remains an open problem ([Steunebrink et al. 2016](http://people.idsia.ch/~steunebrink/Publications/AGI16_growing_recursive_self-improvers.pdf), Mallah 2017). A hypothetical agent created hundreds of years ago, if created with strict value faithfulness and not allowed to update values, would exhibit what one today would consider very odd norms, values, and behavior ([Tegmark 2015](http://www.aaai.org/ocs/index.php/WS/AAAIW15/paper/download/10149/10138)).

### Related Nodes

- [Goal Stability](/Value_Alignment/Foundations/Consistent_Decision_Making/Goal_Stability/Goal_Stability.md)
- [Ontology Update Thresholds](/Value_Alignment/Validation/Increasing_Contextual_Awareness/Realistic_World-Models/Unsupervised_Model_Learning/Ontology_Update_Thresholds/Ontology_Update_Thresholds.md)
	- Reason: From Degree of Value Evolution also see Ontology Update Thresholds since mapping world events to any stale, ungrounded, or nolonger-founded concepts can lead to odd dynamics, and if those concepts are values the effects can be compounded.
- [Unsupervised Model Learning](/Value_Alignment/Validation/Increasing_Contextual_Awareness/Realistic_World-Models/Unsupervised_Model_Learning/Unsupervised_Model_Learning.md)

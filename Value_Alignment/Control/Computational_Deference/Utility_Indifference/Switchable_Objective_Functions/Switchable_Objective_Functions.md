### Mini Description

Techniques to allow an advanced agent to safely accept an alternate utility function from its operators

### Description

Combining objective functions in such a way that the humans have the ability to switch which of those functions an agent is optimizing, but such that the agent does not have incentives to cause or prevent this switch ([Soares et al. 2015](http://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10124/10136), [Armstrong 2010](http://www.fhi.ox.ac.uk/utility-indifference.pdf), [Orseau and Armstrong 2016](http://auai.org/uai2016/proceedings/papers/68.pdf)), would be a key capability ([Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf)).

### Related Nodes

- [Averting Instrumental Incentives](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Averting_Instrumental_Incentives.md)

### Mini Description

Structural methods for operators to maintain control over advanced agents

### Description

It is often desirable to retain some form of meaningful human control ([Russell et al. 2015](http://futureoflife.org/data/documents/research_priorities.pdf)), whether this means a human in the loop or on the loop ([Hexmoor et al. 2009](http://www2.cs.siu.edu/~hexmoor/CV/PUBLICATIONS/JOURNALS/JETAI-08/final.pdf), [Parasuraman et al. 2000](https://hci.cs.uwaterloo.ca/faculty/elaw/cs889/reading/automation/sheridan.pdf)), yet the system should have a clear expectation of whether or not the human is sufficiently experienced, skilled, and ready for what it will ask of them ([Ohn-Bar and Trivedi 2016](http://cvrr.ucsd.edu/eshed/papers/humansTIV.pdf)). It has been argued that very general, capable, autonomous AI systems will often be subject to effects that increase the difficulty of maintaining meaningful human control ([Omohundro 2007](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf), [Bostrom 2012](http://www.nickbostrom.com/superintelligentwill.pdf), [Bostrom 2014](https://global.oup.com/academic/product/superintelligence-9780199678112), [Shanahan 2015](https://mitpress.mit.edu/books/technological-singularity)).

For advanced agents, most goals would actually put the agent at odds with human interests by default, giving it incentives to deceive or manipulate its human operators and to resist interventions designed to change or debug its behavior ([Bostrom 2014](https://global.oup.com/academic/product/superintelligence-9780199678112)). This is because if an AI system is selecting the actions that best allow it to complete a given task, then avoiding conditions that prevent the system from continuing to pursue the task is a natural consequence and can manifest as an emergent subgoal ([Omohundro 2007](http://selfawaresystems.files.wordpress.com/2008/01/nature_of_self_improving_ai.pdf), [Bostrom 2012](http://www.nickbostrom.com/superintelligentwill.pdf)). That could become problematic, however, if one wishes to repurpose the system, to deactivate it, or to significantly alter its decision-making process; such a system would be rational to avoid these changes. Following a broader convention and understanding within the artificial intelligence and computer science domains, self-control and reliable decision making fall under validation while operator control falls in the present section. If methods of alignment and control don't scale with how the intelligence of the system scales, a widening gap of potentially dangerous behavior will appear ([Christiano 2015f](https://medium.com/ai-control/scalable-ai-control-7db2436feee7)). Advanced systems that do not have such issues are termed corrigible systems ([Soares et al. 2015](http://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10124/10136)).

The possibility of rapid, sustained self-improvement has been highlighted by past and current projects on the future of AI as potentially valuable to the project of maintaining reliable control in the long term. Technical work would likely lead to enhanced understanding of the likelihood of such phenomena, and the nature, risks, and overall outcomes associated with different conceived variants ([Horvitz and Selman 2009](https://www.aaai.org/Organization/Panel/panel-note.pdf), [Horvitz 2014](https://ai100.stanford.edu/sites/default/files/ai100_framing_memo_0.pdf)). Theoretical and forecasting work on intelligence explosion and superintelligence have been done before, but require regular updating and improved methods ([Chalmers 2010](http://consc.net/papers/singularity.pdf), [Bostrom 2014](https://global.oup.com/academic/product/superintelligence-9780199678112)). Yet other architectures seek situations that seem unconstrained within some given time horizon ([Wissner-Gross and Freer 2013](http://math.mit.edu/~freer/papers/PhysRevLett_110-168702.pdf)), which present additional control challenges. There will be technical work needed in order to ensure that meaningful human control is maintained for the variety of critical applications (Research 2014) and architectures.

### Related Nodes

- [Averting Instrumental Incentives](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Averting_Instrumental_Incentives.md)
	- Reason: From Control also see Averting Instrumental Incentives therefore, as that contains many of the key prerequisites to reliable control by operators, specifically reliable self-control.

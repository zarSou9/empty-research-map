### Mini Description

Addressing security and integrity issues that arise when systems are made transparent or alternatively seemingly interpretable

### Description

When systems are made transparent, or at least seemingly interpretable, novel security issues can arise. The objective function or the explanatory function may become gameable ([Ghani 2016](http://www.rayidghani.com/you-say-you-want-transparency-and-interpretability)).

### Related Nodes

- [Adversarial ML](/Value_Alignment/Security/Handling_Improper_External_Behavior/Adversarial_ML/Adversarial_ML.md)
	- Reason: From Adversarial Transparency also see Adversarial ML as adversaries may take advantage of transparency to exacerbate such challenges.
- [Open Source Game Theory](/Value_Alignment/Foundations/Consistent_Decision_Making/Decision_Theory/Open_Source_Game_Theory/Open_Source_Game_Theory.md)
	- Reason: From Adversarial Transparency also see Open Source Game Theory because radical transparency in the contexts of creating trustable subagents or successors, or of control of one algorithm by another, can actually lead to much better outcomes for all parties.

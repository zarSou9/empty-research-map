### Mini Description

Methods by which black box or otherwise inscrutible systems can be made more interpretable and their behaviors understandable

### Description

Blackbox interpretability is qualitatively different from whitebox transparency in that individual blackbox decisions are by definition inscrutible. The reporting or visualization of how decisions would have been different given different variations of the inputs, or reporting which of the input's features were most important in making a decision, allows an operator some understanding ([Turner 2016](https://arxiv.org/pdf/1606.09517.pdf), [Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf)) of models in a coarse-grain manner. One can also define coarse abstractions of neural networks that can be more easily verified to satisfy safety constraints ([Pulina and Tacchella 2010](https://pdfs.semanticscholar.org/72e5/5b90b5b791646266b0da8f6528d99aa96be5.pdf)). Methods for explaining classifications that finds a sparse linear approximation to the local decision boundary of a given black-box ML system ([Ribeiro et al. 2016](https://pdfs.semanticscholar.org/2f99/1be8d35e4c1a45bfb0d646673b1ef5239a1f.pdf), [Ribeiro et al. 2016a](http://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf), [Martens and Provost 2014](pages.stern.nyu.edu/%7Efprovost/Papers/MartensProvost_Explaining.pdf)) may also be useful. Similarly, some techniques can report the gradient in the input of the classification judgment ([Baehrens et al. 2010](http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/baehrens10a_%5B0%5D.pdf)), which can be used for exploratory heatmap visualizations of expected behavior. Metrics for reporting the influence of various inputs on the output of a black-box ML system ([Datta et al. 2016](https://www.andrew.cmu.edu/user/danupam/datta-sen-zick-oakland16.pdf), null, [Turner 2016](https://arxiv.org/pdf/1606.09517.pdf)) might be the simplest way to report on why blackbox or proprietary algorithms have reached particular decisions. For deeper explanations, one may consider training systems to output both an answer, such as an action, and a report intended to help the operator evaluate that answer ([Christiano 2016c](https://medium.com/ai-control/the-informed-oversight-problem-1b51b4f66b35)).

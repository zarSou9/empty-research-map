### Mini Description

Techniques to scale human operator oversight of an advanced AI that does a very high volume of very complex actions

### Description

To scale oversight, it is desirable to ensure safe behavior of an agent even if given only limited access to its true objective function ([Christiano 2015f](https://medium.com/ai-control/scalable-ai-control-7db2436feee7)). Methods for efficiently scaling up the ability of human overseers to supervise machine learning systems in scenarios where human feedback is expensive are promising, if early ([Amodei et al. 2016](http://arxiv.org/abs/1606.06565)). One potential approach is semisupervised reinforcement learning, where the agent sees the reward signal for only a small subset of steps or trials ([Amodei et al. 2016](http://arxiv.org/abs/1606.06565), [Christiano 2016f](https://medium.com/ai-control/semi-supervised-reinforcement-learning-cf7d5375197f)).

### Related Nodes

- [Safe Exploration](/Value_Alignment/Validation/Averting_Instrumental_Incentives/Domesticity/Safe_Exploration/Safe_Exploration.md)
- [Scalable Oversight of Ambiguities](/Value_Alignment/Validation/Increasing_Contextual_Awareness/Uncertainty_Identification_and_Management/Inductive_Ambiguity_Identification/Scalable_Oversight_of_Ambiguities/Scalable_Oversight_of_Ambiguities.md)
- [Scaling Judgement Learning](/Value_Alignment/Validation/Technical_Value_Alignment/Robust_Human_Imitation/Scaling_Judgement_Learning/Scaling_Judgement_Learning.md)
	- Reason: From Scalable Oversight also see Scaling Judgement Learning in which such scale is also addressed in a more upfront or potentially offline manner.
- [Unsupervised Model Learning](/Value_Alignment/Validation/Increasing_Contextual_Awareness/Realistic_World-Models/Unsupervised_Model_Learning/Unsupervised_Model_Learning.md)
